


################################################################################
# IN-COUNTRY SEGMENT VULNERABILITY RANKINGS
################################################################################


###################################
# RUN SETUP
source("1_setup.R")
pacman::p_load(corrplot, tidyr, dunn.test)

###################################
# HELPER FUNCTIONS

remove_high_correlation <- function(cor_matrix, threshold = 0.9) {
  while(TRUE) {
    # Count the number of correlations above the threshold for each variable
    count_high_corr <- apply(abs(cor_matrix) > threshold, 2, sum) - 1  # subtract 1 to exclude self-correlation
    # Order the variables by this count
    var_order <- order(count_high_corr, decreasing = TRUE)
    var_to_remove <- colnames(cor_matrix)[var_order[1]]
    # Check if the maximum correlation is within the threshold
    max_cor <- max(abs(cor_matrix[lower.tri(cor_matrix)]))
    if (max_cor <= threshold) break
    # Remove the chosen variable from the correlation matrix
    var_index <- which(colnames(cor_matrix) == var_to_remove)
    cor_matrix <- cor_matrix[, -var_index]
    cor_matrix <- cor_matrix[-var_index, ]
  }
  return(colnames(cor_matrix))
}


create_table <- function(variable) {
  seg <- reshape2::melt(svytable(as.formula(paste0("~class + ", variable)), Pathdesign))
  colnames(seg) <- c("class", "variable", "value")
  df_variable <- seg %>%
    dplyr::filter(!is.na(variable)) %>%
    group_by(class) %>%
    summarise(
      Total_Population = sum(value),
      Count_1 = sum(value*(variable == 1)),
      percent_variable = (Count_1/Total_Population)*100
    ) %>%
    dplyr::select(class, percent_variable)

  return(df_variable)
}


# Min-max scaling function
min_max_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}


###################################
# READ PATHWAYS WORKBOOOK
###################################

outcomes_vars_ranking <- readRDS(outcomes_excel_file) %>%
  dplyr::filter(ranking_include == 1) %>%
  dplyr::select(outcome_variable, short_name) %>%
  distinct()

params_sheet <- readRDS(params_excel_file)

final_models <- params_sheet %>%
  dplyr::select(strata, final_model) %>%
  dplyr::filter(!is.na(strata))

strata_set <- unique(final_models$strata)


###################################
# Read in urban file
urbseg <- readRDS(paste0(lca_path, "urban_outcomes_vulnerability_class.rds"))

# Read in rural file
rurseg <- readRDS(paste0(lca_path, "rural_outcomes_vulnerability_class.rds"))

outcomes_vars <- names(urbseg)[names(urbseg) %in% outcomes_vars_ranking$outcome_variable]

# create combined data frame grabbing 6 classes for urban and 5 for rural
urbseg <- urbseg %>% dplyr::select(LCA6_class, v021, v023, wt, all_of(outcomes_vars)) %>%
  rename(class = LCA6_class) %>%
  mutate(class = paste0("U", class))

rurseg <- rurseg %>% dplyr::select(LCA5_class, v021, v023, wt, all_of(outcomes_vars)) %>%
  rename(class = LCA5_class) %>%
  mutate(class = paste0("R", class))

df <- as.data.frame(rbind(urbseg, rurseg))


# keep binary columns and those that have a less than 30%  missing values
binary_outcomes <- outcomes_vars[sapply(df[outcomes_vars], function(col) all(col %in% c(0, 1, NA)))]

# check fillrates for these columns
na_percentage <- colMeans(is.na(df[binary_outcomes])) * 100
hist(na_percentage)
outcomes_vars <- names(which(colSums(is.na(df[,binary_outcomes]))/nrow(df) < .43))

df <- df %>%
  dplyr::select(class, v021, v023, wt, all_of(outcomes_vars))


# remove highly correlated variable
df_corr <- df %>%
  dplyr::select(all_of(outcomes_vars))
cor_matrix <- cor(df_corr, use = "complete.obs")
corrplot(cor_matrix)
remaining_vars <- remove_high_correlation(cor_matrix)


Pathdesign <- svydesign(id=df$v021, strata = df$v023, weights=df$wt, survey.lonely.psu="adjust", nest=T, data=df)

# Create a table for each variable and store in a list
results_list <- list()
for (i in seq_along(remaining_vars)) {
  variable <- remaining_vars[i]
  results_list[[variable]] <- create_table(variable)
}

combined_df <- purrr::reduce(results_list, full_join, by = "class")
colnames(combined_df)[2:18] <- remaining_vars
colnames(combined_df)[19] <- "stl.yn"

df <- combined_df


# Applying min-max scaling to selected columns
df_scaled <- df
df_scaled[remaining_vars] <- lapply(df[remaining_vars], min_max_scale)a

# Go to long format for plotting
df_scaled_long <- df_scaled %>%
  pivot_longer(cols = -class, names_to = "variable", values_to = "value")


df_scaled_long$class <- as.factor(df_scaled_long$class)

kruskal_result <- kruskal.test(value ~ class, data = df_scaled_long)
print(kruskal_result)

# Dunn's test for pairwise comparisons

if (kruskal_result$p.value < 0.05) {
  dunn_result <- dunn.test(x = df_scaled_long$value,
                           g = df_scaled_long$class,
                           label = TRUE,
                           wrap = TRUE,
                           list = TRUE)
  print(dunn_result)
} else {
  cat("Kruskal-Wallis test not significant, no need for post-hoc pairwise comparisons.\n")
}


# create visual indicating which segments have the most number of significant differences
# with other segments.

classes <- unique(df_scaled_long$class)
df_sig_vals <- data.frame(num_sig_diff = NA, class = classes)

for (i in 1:length(classes)){
  cls <- classes[i]
  idx <- grep(cls, dunn_result$comparisons)
  df_sig_vals$num_sig_diff[i] <- length(which(dunn_result$P[idx] <= .05))
}

df_scaled_long <- left_join(df_scaled_long, df_sig_vals, by = "class")
# order by median
df_scaled_long <- df_scaled_long %>%
  mutate(class = reorder(class, value, FUN = median))


ggplot(df_scaled_long, aes(x = class, y = value, fill = num_sig_diff)) +
  geom_boxplot(outlier.shape = 19, outlier.size = 2, outlier.colour = "black") +
  scale_fill_gradient(low = "white", high = "red") +
  ylab("Normalized health outcomes 0 - best, 1 - worst") +
  xlab("Urban and Rural Segments") +
  coord_flip() +
  theme_minimal() +
  labs(fill = "Statistically\nSignificant\nPairwise\nDifferences",
       title = "Distribution of Normalized Health Outcomes by Segment")


# create a square matrix of Z results for distance matrix and dendagram

labels <- dunn_result$comparisons
split_string <- strsplit(labels, " - ")
lab1 <- sapply(split_string, `[`, 1)
lab2 <- sapply(split_string, `[`, 2)
ZValue = dunn_result$Z
PValue = dunn_result$P

lab1 <- c("R1","R2","R3","R4","R5", "U1","U2", "U3","U4","U5","U6", lab1)
lab2 <- c("R1","R2","R3","R4","R5", "U1","U2", "U3","U4","U5","U6", lab2)
ZValue <- c(rep(0,11), ZValue)


pairwise_matrix <- data.frame(Group1=lab1, Group2=lab2, ZValue=ZValue) %>%
  reshape2::dcast(Group1 ~ Group2, value.var = "ZValue") %>%
  dplyr::select(-Group1) %>%
  as.matrix()

# Since the matrix is symmetric, copy the lower triangle values to the upper triangle
pairwise_matrix[lower.tri(pairwise_matrix)] <- t(pairwise_matrix)[lower.tri(pairwise_matrix)]

# Perform hierarchical clustering
dendrogram <- hclust(as.dist(pairwise_matrix), method = "ward.D2")

# Plot the dendrogram
plot(dendrogram, main = "Hierarchical Clustering of Pairwise Segment Z-scores\n
       from Non Parametric Dunn Test", xlab = "", ylab = "")


# 0-.25 - its gets a 1
# .25-5 it gets a 2
# .5-.75 it gets 3
# .75-1 it gets a 4

median_values <- df_scaled_long %>%
  group_by(class) %>%
  dplyr::summarize(med = median(value)) %>%
  dplyr::mutate(class_rank1 = case_when(med >= 0 & med <= 0.25 ~ 1,
                                        med > 0.25 & med <= 0.5 ~ 2,
                                        med > 0.5 & med <= 0.75 ~ 3,
                                        med > 0.75 & med <= 1 ~ 4)) %>%
  dplyr::mutate(strata = str_sub(class, 1, 1),
                segment = str_sub(class, 2, 2)) %>%
  group_by(strata, class_rank1) %>%
  arrange(med, .by_group = TRUE) %>%
  dplyr::mutate(class_rank2 = dense_rank(med) - 1,
                segment_rank = paste0(strata, class_rank1, ".", class_rank2)) %>%
  ungroup() %>%
  arrange(strata, segment) %>%
  dplyr::select(strata, segment, class, med, class_rank1, class_rank2, segment_rank)
















